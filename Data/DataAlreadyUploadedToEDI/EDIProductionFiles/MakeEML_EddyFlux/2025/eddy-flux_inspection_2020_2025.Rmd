---
title: "EddyFlux Inspection Script"
author: "Adrienne Breef-Pilz"
output: html_document
original date: Jan. 2023
date: "`r Sys.Date()`"
Last edit: 16 Jan. 2024 (ABP)
---


This script is the visual inspection scripts. 

1. QAQCs all the raw data or for a reviewer reads in the data file from EDI for checking. 

2. Then the script checks for duplicates, daily gaps in the file. 

3. Creates plots

4. Writes data to new csv

5. Downloads necessary files for EDI publishing. This file currently does not have a maintenance log



All files are from GitHub or EDI and the source scripts are from GitHub as well. 

If you are REVIEWING this data package, add the pasta URL from EDI in the "READ IN EDI FOR REVIEWER". Make sure eval=FALSE is in the chunk header for "Get raw files", "QAQC all files", "Write CSV" and "Download and save Maintenance Log". You don't need to remake the data files when you are reviewing the data package. These chunks of code will not be run when the R markdown is knitted together and saves you LOTTTS of time. Once that is all set than you can knit the file together as an HTML file to look at all the plots. 

If you are running the code chunk by chunk you mind need to make some adjustments to the code. To make the plots this markdown uses a function called "all_plots". In the function it creates a plots of the most current year, the whole times series, as well as, density and box plots for daily averages. You can also specify if you would like to produce a heat map. Note that heat maps are only produced if your data has a depth column. The function also allows you to choose if you want interactive plots, using plotly, for the most recent year of data. The plotly plots are designed to be in order with the other plots when knit to together. Therefore if you are running the ploting section, chunk by chunk, then you need to specify the plotly plot from the list of plots labeled "output". For this data package do not use plotly for all plots at one time. I usually use it for depth, oxygen, and chla. 

FOR DATA PRODUCT LEAD:

If you are the data product lead and making the data package then:

1. Make sure all the date ranges are up to date especially current_time_start and current_time_end.

2. Change the years in the saved data files in "Write CSV" and "Download and save Maintenance Log" chunk.

3. Make sure to un comment and change the settings for the "Get raw files", "QAQC all files",  and "Write CSV" chunks. Also check the settings for the chunk eval=TRUE or it shouldn't be there. 

4. Comment out the Reviewer section and make sure for the settings eval = FALSE.

5. Knit the file. This will take a long time. 

6. Look over the plots and see if there are any issues that need to be added to the maintenance log. I will usually read in the file you just made and make smaller plots with the variable and month in question. Once I have the dates, add them to the maintenance log. 

7. Re-run the inspection script until you have found all of the issues. 

8. Run the "Download Files" chunk to have the most up to date files in the EDI folder.

9. Make sure large maintenance issues are also documented in the methods.txt file. 



```{r setup packages, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# Load the packages you need
pacman::p_load(tidyverse, lubridate, gsheet, hms,gridExtra,openair, googledrive, knitr, scattermore, htmltools, pander, devtools, plotly)

# Source scripts used in markdown

source("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Scripts/L1_functions/eddy_flux_create.R")


# Plotting function
source('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/EDIProductionFiles/Plotting_function.R')

#turn off pander auto asis
        pander::panderOptions('knitr.auto.asis', FALSE)

# Set up the current time end time of the file and the current year for QAQC plots

#current time of QAQC for graphing
# The flux system is in Eastern time and observes daylight savings
current_time_start=ymd_hms("2025-01-01 00:00:00", tz= "America/New_York")
current_time_end=ymd_hms("2025-12-31 23:59:00", tz= "America/New_York")
```



```{r QAQC raw files, eval=FALSE, include=FALSE}
# QAQC the files we have

# current_df <- eddypro_cleaning_function(
#   directory = "../../../../DataNotYetUploadedToEDI/EddyFlux_Processing/data/",
#    text_file = F,
#   gdrive = T, # Are the files on Google Drive. True or False
#   gshared_drive = as_id("0ACybYKbCwLRPUk9PVA"),
#   output_file = NULL,
#   start_date = as.Date("2020-01-01"),
#   end_date = as.Date("2026-01-01"))

 # Make a DateTime column but remove it before saving the file
#  current_df$DateTime <- ymd_hms(paste0(current_df$date, " " ,current_df$time))

  # force the timezone to Eastern/New_York

  # Set timezone as EST. Streaming sensors don't observe daylight savings
#   current_df$DateTime <- force_tz(current_df$DateTime, tzone = "America/New_York")

   # If No EDI file exists with all the years you want to look at
#  current_df <- current_df%>%
#  dplyr::distinct(.)%>% # get rid of dups if they snuck in
#  filter(DateTime<(current_time_end))

# make sure no time duplicates.
# current_df<-  current_df[!duplicated(current_df$DateTime), ]

#reorder. Just to be certain everything is in order
#  current_df<-current_df[order(current_df$DateTime),]


```

REVIEWERS- If you are reviewing this data package replace the pasta link with the one from EDI. If there are questions ask the data point person. 

```{r READ IN EDI FOR REVIEWER, include=FALSE}

# For REVIEWERS: Run this section to pull the data from EDI which is in staging as a check of the data. The pasta link from EDI has already been updated for the year 2026.
    
current_df <-read_csv("https://pasta-s.lternet.edu/package/data/eml/edi/692/21/9686456c091a205d2444fb01342eeedc")

#Force files from EDI to have an EST timestamp

#Make a DateTime column but remove it before saving the file
 current_df$DateTime <- ymd_hms(paste0(current_df$date, " " ,current_df$time))

#Set timezone as EST. Streaming sensors don't observe daylight savings
  current_df$DateTime <- force_tz(current_df$DateTime, tzone = "America/New_York")

```


## Check for duplicates and  gaps in the data frame

This section identifies if there are any duplicates, and daily gaps in the long-term record. If there are duplicates. Look to see if they are true duplicates and then check the qaqc function and the chunk above where duplicates should be removed. 


### Are there any duplicates?
Note: If nothing shows up, that means there are no duplicates in the current dataset.

```{r Check for dups , echo=FALSE}

# Make sure there are no duplicated dates. Do this here because the file is too large for Data Explore.
# Print them if there are
 dups<- current_df[duplicated(current_df$DateTime), ]

dups <- dups%>%
  select(DateTime, co2_flux_umolm2s,  specific_humidity_kgkg, u_star_ms)

# Make it into a nice table when the Markdown is knitted together
knitr::kable((dups))
```


### Are there any gaps in the data file?


When gaps are found in the data file, check that you do not have new gaps in the previous years' publication. For the current year, if you find gaps check that you have all of the manually downloaded files. If the data are truly missing then record the dates and times in the methods section. 

These are the data gaps when the entire system was down and/or data isn't available.

```{r echo=FALSE}
#######
#Detect rows with all NAs except the qc and datetime columns
rows_with_all_NAs_except <- current_df %>%
  filter(rowSums(is.na(select(., -qc_Tau, -qc_H, -qc_LE, -qc_co2_flux, -qc_h2o_flux,   -qc_ch4_flux, -date, -time, -DOY, -DateTime))) != ncol(select(., -qc_Tau, -qc_H, -qc_LE, -qc_co2_flux, -qc_h2o_flux,   -qc_ch4_flux, -date, -time, -DOY, -DateTime)))

current_df <- rows_with_all_NAs_except

# Assign the current_df dataset to df2 (so that current_df remains unchanged for use later) and get DOY
df2 <- current_df
 df2$DOY=yday(df2$DateTime)

for(i in 2:nrow(df2)){ #this identifies if there are any data gaps in the long-term record, and where they are by record number
   if(df2$DOY[i]-df2$DOY[i-1]>1){
     print(c(df2$DateTime[i-1],df2$DateTime[i]))
   }
}
######
 
```

These are the gaps longer than a day only for carbon dioxide flux data.

```{r echo=FALSE}
######
#To detect gaps longer than a day only in CO2 column
na_count <- 0

for (i in 1:nrow(df2)) {

  if (is.na(df2$co2_flux_umolm2s[i])) {
    na_count <- na_count + 1
  } else {
    # If a NA run just ended, check its length
    if (na_count > 48) {
      gap_start <- df2$DateTime[i - na_count]
      gap_end   <- df2$DateTime[i - 1]

      print(c(gap_start, gap_end))
    }
    na_count <- 0
  }
}

# Catch case where file ends during a gap (This is to make sure that final gap isn't missed entirely due to no non-NA value after the last row to trigger the else block. This extra check makes sure that the final NA block gets counted)
if (na_count > 48) {
  gap_start <- df2$DateTime[nrow(df2) - na_count + 1]
  gap_end   <- df2$DateTime[nrow(df2)]

  print(c(gap_start, gap_end))
}

```


These are the gaps longer than a day only for methane flux data.

```{r Check for daily gaps, echo=FALSE}

#HOWEVER, sometimes one of the sensors is working and the other is not, due to which the DOY column doesn't represent a true gap in data. In that case, we'll have to check the CO2 and CH4 columns separately and list them out as missing data in different times. There's a big jump in DateTime column when the system is down, however, the days appear serially when only one of the sensors appears not working. This is the reason we need three different code chunks to check whether the entire system or only one of the sensors was down.


########

na_count <- 0

for (i in 1:nrow(df2)) {

  if (is.na(df2$ch4_flux_umolm2s[i])) {
    na_count <- na_count + 1
  } else {
    # If a NA run just ended, check its length
    if (na_count > 48) {
      gap_start <- df2$DateTime[i - na_count]
      gap_end   <- df2$DateTime[i - 1]

      print(c(gap_start, gap_end))
    }
    na_count <- 0
  }
}

# Catch case where file ends during a gap (This is to make sure that final gap isn't missed entirely due to no non-NA value after the last row to trigger the else block. This extra check makes sure that the final NA block gets counted)
if (na_count > 48) {
  gap_start <- df2$DateTime[nrow(df2) - na_count + 1]
  gap_end   <- df2$DateTime[nrow(df2)]

  print(c(gap_start, gap_end))
}
 
#####

```

## QAQC Plots

##### QAQC plot information and all_plot function information

For the plots, they use a function called "all_plot". In all_plot you can specify if you want interactive plots for the current data. You can specify which plotly plots you want on. You can also look at the plotly plots manually in each chunk by running the chunk with Use_plotly=TRUE as an argument. Then look at the list of plots you have made under "output". If you click on the "output" object in your environment it will list all of the plots and which ones are interactive plotly plots. To view the plots run "output[[number of the plot in the list you want to run]]". Eg. If you want to see the 4th plot in the list write output[[4]] then run that line and the plot should appear. 

If you would like to look at one variable then in the function below replace "dx" with the variable column. The object "dx" is just a list of variables so we don't have to list out all of the variables we want to plot. This is used to speed up the process. 

The plotting function is called all_plot() which plots the 4 or more plots described below. The function is sourced from GitHub in the first chunk of the script. The arguments are:
Var, # the variable you would like to plot
data, # the data frame to use
raw_data=NULL, # Is there raw data to compare with. Usually is NULL
reservoir, # the name of the reservoir you would like to filter by 
res_site, # the reservoir Site or Sites you would like to filter by
y_lab,  # This label can take an expression aka have the proper degrees C, 
y_lab2, # This label is for the plotly function which can not handle expression argument. 
Depth=F,  # Do you want depth as a factor
Water=T, # Are these plots for streaming sensors with RDO and temperature sensors
Use_plotly = F, # Do you want to produce interactive plots for observations of the current year?
Heatmap = F) # Do you want to make a heat maps? This only works if there are multiple depths at the same site


The arguments with = followed by a True means that they are the defaults and you don't need to add them to the function when you use it. If you want to use the opposite of the default you must specify that. 
  
##### Plot Description:

The plots below are:
The first 2 plots are the ones you should focus on for the QAQC chec. Spend the most time looking at the most recent data because that one as been checked. Do pay attention to the historical to make sure there are no crazy outliers that were missed in previous years. 

1. A time series of the current years' data. This can either be a regular plot or an interactive one. If you would like it to be interactive set Use_plotly = T. The black dots are the qaqced observations and the red is the raw files that were qaqced. This is to see what kind of values were removed and if there are any the script missed or that need to be added to the maintenance log. 

2. A time series of the historical and the current data just the qaqced values. 

The next two plots are just fun to see trends over time with the data. 

3. Density plots are like a histogram and a grouped by color so you can see where the data are relative to other years. 

4. The box plots look at the spread of the data within the year and we can look at the median and see how that is changing or not. 

Do not over think the last 2 plots, although they are good to look at historical trends to ensure relative data quality consistency. 

### Flux Plots for CO2, H2O, and CH4

```{r Fluxes, echo=FALSE, warning=FALSE, results='asis'}

dx <- colnames(current_df%>%select(co2_flux_umolm2s, h2o_flux_umolm2s, ch4_flux_umolm2s))

# make the plots
outputs <- lapply(dx, all_plot,data=current_df, raw_data=NULL, reservoir=NULL, res_site =NULL, y_lab="umolm2s", y_lab2="umolm2s",Depth=F, Water=F, Use_plotly=F, Heatmap=F)

output <- unlist(outputs, recursive = F)

output[[1]]
```

```{r Print plotly fluxes, echo=FALSE, warning=FALSE, messages=FALSE}

 # Used to print the plotly plots
  # attach the Dependencies
  # since the do not get included with renderTags(...)$html
  deps <- lapply(
    Filter(function(x){inherits(x,"htmlwidget")},output),
    function(hw){
      renderTags(hw)$dependencies
    }
  )
  
  if(length(deps)>0){
  attachDependencies(
    tagList(),
    unlist(deps,recursive=FALSE)
  )
  }  
```


### Compare 30 minute flux to daily flux

This is a similar figure to the one in Alex's paper. The 30 minute fluxes are black dots and the line is the average daily flux

#### CO2

```{r CO2 Thirty Minute to Daily fluxes, echo=FALSE}
daily <- current_df%>% 
  group_by( Date = as.Date(DateTime)) %>% 
  summarise_if(is.numeric, mean, na.rm=T)%>%
  mutate(Year = as.factor(year(Date)),
         Month = month(Date),
         Time = "12:00:00")%>%
  mutate(DateTime= paste0(Date, Time, sep=" "))%>%
  mutate(DateTime=ymd_hms(DateTime)) 

# current daily average 
a <- daily %>%
  filter(DateTime>current_time_start)

# current file
current <- current_df %>%
  filter(DateTime>current_time_start & DateTime<current_time_end)


  ggplot()+
  geom_point(data=current, aes(x= DateTime, y= co2_flux_umolm2s*60*60*24*44.01/1e6),alpha = 0.1)+
  geom_hline(yintercept = 0, linetype="dashed")+
  geom_line(data = a, aes(x=DateTime, y=co2_flux_umolm2s*60*60*24*44.01/1e6),color="#E63946", linewidth = 1)+
  theme_classic(base_size = 15)+
  xlab("DateTime") + ylab(expression(~CO[2]~flux~(g~m^-2~d^-1))) +
  ggtitle("Current Year Thirty Minute Fluxes to Daily Fluxes")


ggplot()+
  geom_point(data = current_df, aes(x= DateTime, y= co2_flux_umolm2s*60*60*24*44.01/1e6),alpha = 0.1)+
  geom_hline(yintercept = 0, linetype="dashed")+
  geom_line(data = daily, aes(x=DateTime, y=co2_flux_umolm2s*60*60*24*44.01/1e6),color="blue", linewidth = 1)+
  theme_classic(base_size = 15)+
  xlab("DateTime") + ylab(expression(~CO[2]~flux~(g~m^-2~d^-1))) +
  ggtitle("Thirty Minute Fluxes to Daily Fluxes")

```

#### CH4 

```{r CH4 Thirty Minute to Daily fluxes, echo=FALSE}

ggplot()+
  geom_point(data = current, aes(x= DateTime, y= ch4_flux_umolm2s),alpha = 0.1)+
  geom_hline(yintercept = 0, linetype="dashed")+
  geom_line(data = a, aes(x=DateTime, y=ch4_flux_umolm2s),color="#E63946", linewidth = 1)+
  theme_classic(base_size = 15)+
  xlab("DateTime") + ylab(expression(~CH[4]~flux~(g~m^-2~d^-1))) +
  ggtitle("Current Year Thirty Minute Fluxes to Daily Fluxes")


ggplot()+
  geom_point(data = current_df, aes(x= DateTime, y=ch4_flux_umolm2s*60*60*24*16.04/1e6),alpha = 0.1)+
  geom_hline(yintercept = 0, linetype="dashed")+
  geom_line(data = daily, aes(x=DateTime, y=ch4_flux_umolm2s*60*60*24*16.04/1e6),color="blue", linewidth = 1)+
  theme_classic(base_size = 15)+
  xlab("DateTime") + ylab(expression(~CH[4]~flux~(g~m^-2~d^-1))) +
  ggtitle("Thirty Minute Fluxes to Daily Fluxes")

```

### Wind Rose plots for the Current Year and All Observations

Plot the wind direction, see what it looks like. 

```{r Wind Rose, echo=FALSE}

# Visualize wind directions for just the current year 
      chicago_wind <- current%>%
        select(DateTime,wind_speed_ms,wind_dir)%>%
        dplyr::rename(date = DateTime, ws = wind_speed_ms, wd = wind_dir)

    # Make Plot
      pollutionRose(chicago_wind, pollutant="ws")
      
      
      
 # Visualize wind directions for all obs      
      chicago_wind <- current_df%>%
        select(DateTime,wind_speed_ms,wind_dir)%>%
        dplyr::rename(date = DateTime, ws = wind_speed_ms, wd = wind_dir)
      
      # Make plot
      pollutionRose(chicago_wind, pollutant="ws", type = "year")
      
```

### Ancillary Data

U_star_ms is friction velocity

sonic temperature is temperature in kelvin from the sonic anemometer

H_wm2 is Sensible heat flux

LE_wm2 is Latent heat flux

```{r other data, echo=FALSE, warning=FALSE, results='asis'}

dx <- colnames(current_df%>%select(u_star_ms, sonic_temperature_k, H_wm2, LE_wm2))

# make the plots
outputs <- lapply(dx, all_plot,data=current_df, raw_data=NULL, reservoir=NULL, res_site =NULL, y_lab="ms/K/wm2", y_lab2="ms/K/wm2",Depth=F, Water=F, Use_plotly=T, Heatmap=F)

output <- unlist(outputs, recursive = F)

output[[5]]
```

```{r Print plotly other data, echo=FALSE, warning=FALSE, messages=FALSE}

 # Used to print the plotly plots
  # attach the Dependencies
  # since the do not get included with renderTags(...)$html
  deps <- lapply(
    Filter(function(x){inherits(x,"htmlwidget")},output),
    function(hw){
      renderTags(hw)$dependencies
    }
  )
  
  if(length(deps)>0){
  attachDependencies(
    tagList(),
    unlist(deps,recursive=FALSE)
  )
  }  

```

#### Write the file to csv

```{r Make new CSV with current and historic files, eval=FALSE, include=FALSE}

# Take out the DateTime column and make date and time column characters
current_df2 <- current_df%>%
  select(-DateTime)%>%
  mutate(date = as.character(date),
         time = as.character(time))


# Need to decide on a naming convention for this file
write_csv(current_df2, "eddy-flux_2020_2025.csv")

```

```{r Download Files, eval=FALSE, include=FALSE}

# save qaqc file in the folder
download.file("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Scripts/L1_functions/eddy_flux_create.R", "eddy-flux_qaqc_2020_2025.R")

# save plotting function in folder
download.file("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/EDIProductionFiles/Plotting_function.R", "Plotting_function")

# save processing script in folder
download.file("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/FCR_Process_BD_Markdown.Rmd", "eddy-flux_processing_2020_2025.Rmd")

# Despike function for EddyFlux_processing_2020_2023.Rmd
download.file("https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataNotYetUploadedToEDI/EddyFlux_Processing/despike.R", "despike.R")

```


