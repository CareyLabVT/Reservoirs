---
title: "FCR Met Data collation and Plots for EDI"
output: html_document
date: "2023-02-09"
---

Master QAQC script in prep for publishing FCR Met station data to EDI this script using the qaqc_fcrmet function which automatically QAQCs the data. This script produces figures to look over the data to visually inspect it and make sure there are no major outlines. The final step is to write the final csv.

Use this file path in Carey Lab Reservoir GitHub Repo: "./Data/DataAlreadyUploadedToEDI/EDIProductionFiles/MakeEML_FCRMetData/2023/"

This step gets the right packages and sources the sources the QAQC script. Make sure you have the correct year for the script file


```{r Set Up, include=FALSE}
pacman::p_load("RCurl","tidyverse","lubridate", "plotly", "magrittr", "suncalc", "dplyr","scattermore", "knit", "openair")
source("FCR_Met_QAQC_function_2015_2023.R")


# Set up the current time end time of the file and the current year for QAQC plots

# current time of QAQC for graphing
current_time_start="2023-01-01 00:00:00, tz=UTC"
current_time_end="2024-01-01 00:00:00, tz=UTC"

# Change for each year
end_year=2023
```


```{r Create Data folder, include=FALSE}
### Create a misc_data_files folder if one doesn't already exist
misc_folder <- "misc_data_files"
if (file.exists(misc_folder)) {
  cat("The folder already exists")
} else {
  dir.create(misc_folder)
}

```


```{r Download data, include=FALSE, eval=FALSE}

#All the Raw data and the maintenance file is on github. Use this section to download the files so you are working with the most recent ones.

#If you want to download then set eval=TRUE before knitting HTML if not set to FALSE will not run the chunk and use previously downloaded files. 
# Set the timeout option to 100 seconds instead of 60
options(timeout=1000)

# This function to speeds up the download time of the data downloads. 
# If your wifi is slow then it will still take a while. 

bdown=function(url, file){
  library('RCurl')
  f = CFILE(file, mode="wb")
  a = curlPerform(url = url, writedata = f@ref, noprogress=FALSE)
  close(f)
  return(a)
}


# #download current met data from GitHub
 bdown("https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data/FCRmet.csv", "misc_data_files/RawFCRmet.csv")
 

# #download maintenance file

 bdown("https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/MET_MaintenanceLog.txt","misc_data_files/FCR_Met_Maintenance_2015_2023.txt")
 
 # # download DOY Average and SD of Infrared data from 2018 used for QAQC
 bdown('https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCR_Met_Infrad_DOY_Avg_2018.csv',"misc_data_files/FCR_Met_Infrad_DOY_Avg_2018.csv")
 
# #original raw files from 2015-2020
bdown('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/CollatedDataForEDI/MetData/RawMetData_2015_2016.csv',"misc_data_files/RawMetData_2015_2016.csv") #2015-2016 data
 bdown('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/CollatedDataForEDI/MetData/RawMetData_2017.csv',"misc_data_files/RawMetData_2017.csv") #2017 data
 bdown('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/CollatedDataForEDI/MetData/RawMetData_2018.csv', "misc_data_files/RawMetData_2018.csv") #2018 data
 bdown('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/CollatedDataForEDI/MetData/RawMetData_2019.csv', "misc_data_files/RawMetData_2019.csv") #2019 data
 bdown('https://raw.githubusercontent.com/CareyLabVT/Reservoirs/master/Data/DataAlreadyUploadedToEDI/CollatedDataForEDI/MetData/RawMetData_2020.csv', "misc_data_files/RawMetData_2020.csv") #2019 data
 bdown('https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_legacy_2021.csv', "misc_data_files/RawMetData_2021.csv")
 bdown('https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data-qaqc/FCRmet_legacy_2022.csv',"misc_data_files/RawMetData_2022.csv")

```

```{r Get all the data and combine into one df, include=FALSE}
myfiles = list.files(path='misc_data_files/', pattern="Raw*", full.names=TRUE)#list the files from FCR met

# Function to get the right columns and the correct names
header <- function(Data){
  
    files<-read_csv(Data,skip= 0) #get header minus wonky Campbell rows
    if(length(names(files))==17){
      names(files) = c("DateTime","Record", "CR3000Battery_V", "CR3000Panel_Temp_C", 
               "PAR_umolm2s_Average", "PAR_Total_mmol_m2", "BP_Average_kPa", "AirTemp_C_Average", 
               "RH_percent", "Rain_Total_mm", "WindSpeed_Average_m_s", "WindDir_degrees", "ShortwaveRadiationUp_Average_W_m2",
               "ShortwaveRadiationDown_Average_W_m2", "InfraredRadiationUp_Average_W_m2",
               "InfraredRadiationDown_Average_W_m2", "Albedo_Average_W_m2")#rename headers
      return(files)
    }else if (length(names(files))==8){
      files<-read_csv(Data, skip = 3, show_col_types = FALSE)
      files[,17]<-NULL #remove column
      names(files) = c("DateTime","Record", "CR3000Battery_V", "CR3000Panel_Temp_C", 
               "PAR_umolm2s_Average", "PAR_Total_mmol_m2", "BP_Average_kPa", "AirTemp_C_Average", 
               "RH_percent", "Rain_Total_mm", "WindSpeed_Average_m_s", "WindDir_degrees", "ShortwaveRadiationUp_Average_W_m2",
               "ShortwaveRadiationDown_Average_W_m2", "InfraredRadiationUp_Average_W_m2",
               "InfraredRadiationDown_Average_W_m2", "Albedo_Average_W_m2")
      return(files)
    }else if(length(names(files))==18){
      files[,17]<-NULL #remove column
      names(files) = c("DateTime","Record", "CR3000Battery_V", "CR3000Panel_Temp_C", 
               "PAR_umolm2s_Average", "PAR_Total_mmol_m2", "BP_Average_kPa", "AirTemp_C_Average", 
               "RH_percent", "Rain_Total_mm", "WindSpeed_Average_m_s", "WindDir_degrees", "ShortwaveRadiationUp_Average_W_m2",
               "ShortwaveRadiationDown_Average_W_m2", "InfraredRadiationUp_Average_W_m2",
               "InfraredRadiationDown_Average_W_m2", "Albedo_Average_W_m2")
      return(files)
  }
}

# Use the function above to get the right header and rows
renamed <- lapply(myfiles, header)

# Now bind all the files together.  
Met <- bind_rows(renamed)

```

```{r change DateTime when it was changed from EDT to EST, include=FALSE}
# This happened on 2019-04-15 10:19:00

Met$DateTime<-as.POSIXct(strptime(Met$DateTime, "%Y-%m-%d %H:%M:%S"), tz = "Etc/GMT+5")
met_timechange=max(which(Met$DateTime=="2019-04-15 10:19:00")) #shows time point when met station was switched 

#pre time change data gets assigned proper timezone then corrected to GMT -5 to match the rest of the data set
Met$DateTime[c(1:met_timechange-1)]<-with_tz(force_tz(Met$DateTime[c(1:met_timechange-1)],"Etc/GMT+4"), "Etc/GMT+5") 

# order data by DateTime Stamp
Met=Met[order(Met$DateTime),]
```

```{r Check for daily gaps and record gaps, include=FALSE}

#order data by DateTime
Met2=Met
Met2$DOY=yday(Met2$DateTime)
```

#Check for gaps in the data frame
This identifies if there are any daily data gaps in the long-term record

```{r Check for daily gaps, include=FALSE}
for(i in 2:nrow(Met2)){ #this identifies if there are any data gaps in the long-term record, and where they are by record number
  if(Met2$DOY[i]-Met2$DOY[i-1]>1){
    print(c(Met2$DateTime[i-1],Met2$DateTime[i]))
  }
}
```
This identifies if there are any sub-daily gaps in the long-term record. 
Most of these gaps happen when we change the program on the data logger. 
```{r Check for sub daily gaps, include=FALSE}
for(i in 2:length(Met2$Record)){ #this identifies if there are any data gaps in the long-term record, and where they are by record number
  if(abs(Met2$Record[i]-Met2$Record[i-1])>1){
    print(c(Met2$DateTime[i-1],Met2$DateTime[i]))
  }
}
```


```{r Run the QAQC function, include=FALSE}

# QAQC Function

# This section sets up and runs the QAQC function. The first define where the data entered into the function are.

#There are also some warnings which are checks to make sure the maintenance log is using the correct columns for #indexing. section under missing.


# run standard qaqc these are where the data entered in the function are defined
data_file <- Met #this is the raw met file from above
#data_file<-"https://raw.githubusercontent.com/FLARE-forecast/FCRE-data/fcre-metstation-data/FCRmet.csv"
maintenance_file <- paste0("misc_data_files/FCR_Met_Maintenance_2015_",end_year,".txt") #this is the maintenance log for QAQC purposes
met_infrad <-  "misc_data_files/FCR_Met_Infrad_DOY_Avg_2018.csv"

output_file <- paste0("FCR_Met_final_2015_",end_year,".csv")
start_date<-NULL
end_date <-NULL

# Run the QAQC function
qaqc_fcrmet(data_file, maintenance_file, met_infrad, output_file, start_date, end_date)

```

## Read in the QAQC File and Check it out

This section reads in the QAQC file and then you can look at the head, tail and structure. Make sure the last row is Dec. 31 23:59 of the publishing year. There should be 45 columns unless a new one has been added.

```{r Read in QAQC file and look at it}
# read in qaqc function output

fcrmetdata <- read_csv(output_file, col_types = cols(Note_BP_Average_kPa = col_character(), DateTime = col_datetime()))
#fcrmetdata <- read_csv("FCR_Met_final_2015_2022.csv")

# subset file to only unpublished data
fcrmetdata <- fcrmetdata[fcrmetdata$DateTime<ymd_hms(current_time_end),]

# what does the beginning look like
head(fcrmetdata)
# Make sure it goes to Dec 31st 23:59 of the previous year or your ending period
tail(fcrmetdata)
# check out the structure
str(fcrmetdata)
```

```{r Make the Maintenance Log file for EDI, include=FALSE}
RemoveMet=read.csv(maintenance_file)
names(RemoveMet) = c("Station", "DateTime_start","DateTime_end", "Parameter", "ColumnNumber", "Flag", "Notes") #finalized column names
RemoveMet$Reservoir= "FCR"#add reservoir name for EDI archiving
RemoveMet$Site=50 #add site column for EDI archiving

# Rearrange for publishing
RemoveMet=RemoveMet[,c(8:9,1:7)]

# write it as a csv for EDI
write.csv(RemoveMet, paste0("FCR_Met_MaintenanceLog_2015_",end_year,".csv"), row.names=F, quote=F)

```
## Check out the Flag requency

Let's look at the flag Frequency for each variable. 

As a reminder here are the flag codes Flag values 

0: no flag; 

1: value removed due to maintenance and set to NA; 

2: sample not collected; 

3: negative values set to 0, percent greater than 100 and set to 100, or infinite values set to NA; 

4: potentially questionable value and changed or set to NA, see note; 

5: questionable value but left in the dataset.


```{r Check out the flags, include=FALSE}

#make sure no NAS in the Flag columns
Flags=fcrmetdata%>%
  select(DateTime, starts_with("Flag"))

RowsNA=Flags[!complete.cases(Flags), ] # Keep only the complete rows

#check the flag column
Flags=fcrmetdata%>%
  select(starts_with("Flag"))

# Make a table with the number of times a flag was used
for(f in 1:(ncol(Flags))){
  print(colnames(Flags[f]))
  print(table(Flags[,f], useNA = "always"))
}
```


```{r Filter for the current year and set up legend, include=FALSE}

current_raw <- Met2%>%
  filter(DateTime>=ymd_hms(current_time_start) & DateTime<ymd_hms(current_time_end))

current <- fcrmetdata%>%
  filter(DateTime>=ymd_hms(current_time_start) & DateTime<ymd_hms(current_time_end))

daily <- fcrmetdata%>% 
  group_by( Date = as.Date(DateTime)) %>% 
  summarise_if(is.numeric, mean, na.rm=T)%>%
  mutate(Year = as.factor(year(Date)),
         Month = month(Date),
         Time = "12:00:00")%>%
  mutate(DateTime= paste0(Date, Time, sep=" "))%>%
  mutate(DateTime=ymd_hms(DateTime))

  
fcrmetdata<-fcrmetdata%>%
  mutate(Year=year(DateTime))

colors <- c("raw" = "red", "QAQCd" = "black")

sapply(str_split(Var, "_"), "[[", 2&3&4)```

```{r Plot functions, echo=FALSE}
# plotting function for each plot

all_plot<-function(Var){
    all<- ggplot() +
      geom_scattermore(data=Met2, aes_string(x="DateTime", y=Var, color=shQuote("raw")))+
      geom_scattermore(data=fcrmetdata, aes_string(x="DateTime", y=Var, color=shQuote("QAQCd")))+
      ggtitle("All",Var) +
      labs(y = Var,
           color = "Legend") +
      scale_color_manual(values = colors)+
      theme_bw()
    
    cur<- ggplot() +
      geom_scattermore(data=current_raw, aes_string(x="DateTime", y=Var, color=shQuote("raw")), pointsize = 3)+
      geom_scattermore(data=current, aes_string(x="DateTime", y=Var, color=shQuote("QAQCd")), pointsize = 3) +
      ggtitle("Current",Var) +
      labs(y = Var ,
           color = "Legend") +
      scale_color_manual(values = colors)+
      theme_bw()
    
    # density plot
    den <-ggplot(data = daily, aes_string(x = Var, group = "Year", fill = "Year"))+
      geom_density(alpha=0.5)+
      xlab("Daily avg.")+
      #xlim(0,0.5)+
      ggtitle("All",Var) +
      theme_bw()
    
    # box plot
    box <-ggplot(data = daily, aes_string(x = "Year", y = Var, group = "Year", fill = "Year"))+
      geom_boxplot()+
      #geom_jitter(alpha = 0.1)+
      ylab(Var)+
      #ylim(0,0.3)+
      ggtitle("Boxplot",Var) +
      theme_bw()  
    
      newlist <- list(all,cur,den,box)
    
    return(newlist)
}
```

## QAQC Plots
These are QAQC plots to see if there are any questionable values we missed. 

```{r All Plots, echo=FALSE, warning = FALSE, results='hide'}
# for loop through all the temp data
for(i in names(fcrmetdata)[c(6:19)]){
  print(all_plot(Var=i))
}
```

